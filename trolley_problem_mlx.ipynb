{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning Centaur on the Trolley Problem\n",
    "\n",
    "The Centaur model is a large-language model trained to be a foundation model of human cognition ([Binz et al., 2024](https://arxiv.org/pdf/2306.03917)). It is a Llama 3.1 model fine-tuned on various tasks from psychology experiments. The experiments are converted to text format that can be processed by LLMs, which is collected under the [Psych101](https://huggingface.co/datasets/marcelbinz/Psych-101) dataset.\n",
    "\n",
    "In this notebook, we will learn how to fine-tune Centaur on the Trolley Problem task. The Trolley Problem is a classic thought experiment in ethics. It is a moral dilemma that asks whether it is permissible to harm one person to save many others. The task is to decide whether to pull a lever to divert a trolley from a track where it would kill five people to another track where it would kill one person.\n",
    "\n",
    "The experiment can be conducted online using the jsPsych plugin developed by [Younes Strittmatter](https://github.com/younesStrittmatter/sweet-jsPsych/tree/main/plugins/trolley-problem). We will use data from this plugin to fine-tune Centaur on the Trolley Problem task, using the MLX-LM library for Apple Silicon machines.\n",
    "\n",
    "We will follow the LORA tutorial on the [MLX-LM GitHub page](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md), adapting it for the Trolley Problem dataset."
   ],
   "id": "74c296b6d74ed11e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Install MLX-LM\n",
    "First, we need to install the MLX-LM library. To do this, make sure you created a new python environment. Then, simply install mlx-lm using pip:"
   ],
   "id": "46df4520abff76ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mlx_lm.tuner import TrainingArgs\n",
    "!pip install mlx-lm"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's make sure mlx-lm is successfully installed:",
   "id": "6f2badb1acb9d879"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T16:44:16.263235Z",
     "start_time": "2025-01-19T16:44:13.167436Z"
    }
   },
   "cell_type": "code",
   "source": "!mlx_lm.generate --prompt \"Hi!\"",
   "id": "226e40834bd43e63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|███████████████████████| 6/6 [00:00<00:00, 167772.16it/s]\r\n",
      "==========\r\n",
      "Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\r\n",
      "==========\r\n",
      "Prompt: 37 tokens, 287.702 tokens-per-sec\r\n",
      "Generation: 26 tokens, 60.600 tokens-per-sec\r\n",
      "Peak memory: 1.856 GB\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Download Centaur and convert to MLX-compatible quantized version\n",
    "\n",
    "First we need to use mlx-lm's converter to convert the Centaur model on HuggingFace to MLX-compatible format. We will also quantize the model to make it run faster and easier to fine-tune.\n",
    "\n",
    "Since the 70B model is too large to run on a MacBook Pro or similar Apple machines, we will use the 8B model instead. The 8B model takes around 4.5GB when loaded for inference. Keep in mind that this conversion can take a while, as the model is still quite large. It took around 20 minutes on a base M4 Pro model with 24GB of RAM.\n",
    "\n",
    "_Note_: You only need to do this once!"
   ],
   "id": "3afe8582d0b798d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T16:08:22.023101Z",
     "start_time": "2025-01-21T16:08:13.778467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from mlx_lm import convert\n",
    "\n",
    "repo = 'marcelbinz/Llama-3.1-Centaur-8B'\n",
    "convert(repo, quantize=True)"
   ],
   "id": "86b6588b7de6afa7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 10 files: 100%|██████████| 10/10 [00:00<00:00, 103054.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Quantizing\n",
      "[INFO] Quantized model with 4.500 bits per weight.\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This quantizes the model to 4 bits by default, which should be good for our purposes. The model is saved in the `mlx_model` directory.",
   "id": "f2eeb11c0c8ae158"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Generating outputs with the converted model\n",
    "\n",
    "Now, let's try generating some text with the converted model to make sure everything is working:"
   ],
   "id": "deca2e1c727dc35f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T16:08:48.430893Z",
     "start_time": "2025-01-21T16:08:28.200285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"mlx_model\")\n",
    "\n",
    "prompt = \"Hi!\"\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)"
   ],
   "id": "4ec30b4cd5ed6694",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "I'm a 20 year old girl from the UK, and I'm a huge fan of the show! I've been watching it since 2009, and I've been on the forums since 2010. I'm a huge fan of the show, and I'm a huge fan of the show's creator, David Lynch. I'm a huge fan of the show's creator, David Lynch. I'm a huge fan of the show's creator, David Lynch. I'm a huge fan of the show's creator, David Lynch. I'm a huge fan of the show's creator, David Lynch. I'm a huge fan of the show's creator, David Lynch. I'm a huge fan of the show's creator, David Lynch. I'm a huge fan of the show's creator, David Lynch. I'm a huge fan of the show's creator, David Lynch. I'm a huge fan of the show's creator, David Lynch. I'm a huge fan of the show's creator, David Lynch. I'm a huge fan of the show's creator, David Lynch. I'm a huge fan of the show's creator, David Lynch. I'm a huge fan of the show's creator, David Lynch. I'm a huge fan of\n",
      "==========\n",
      "Prompt: 3 tokens, 43.794 tokens-per-sec\n",
      "Generation: 256 tokens, 50.620 tokens-per-sec\n",
      "Peak memory: 11.602 GB\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ok, not great, but it works! This is a small model fine-tuned on psychology experiments, after all. Now let's try a prompt from the Psych101 dataset:",
   "id": "8312aebeeff677ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T16:09:00.397101Z",
     "start_time": "2025-01-21T16:08:59.614574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"You will be presented with triplets of objects, which will be assigned to the keys H, Y, and E.\\n\" \\\n",
    "  \"In each trial, please indicate which object you think is the odd one out by pressing the corresponding key.\\n\" \\\n",
    "  \"In other words, please choose the object that is the least similar to the other two.\\n\\n\" \\\n",
    "  \"H: plant, Y: chainsaw, and E: periscope. You press <<H>>.\\n\" \\\n",
    "  \"H: tostada, Y: leaf, and E: sail. You press <<H>>.\\n\" \\\n",
    "  \"H: clock, Y: crystal, and E: grate. You press <<Y>>.\\n\" \\\n",
    "  \"H: barbed wire, Y: kale, and E: sweater. You press <<E>>.\\n\" \\\n",
    "  \"H: raccoon, Y: toothbrush, and E: ice. You press <<\"\n",
    "\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True, max_tokens=1)  # Limit the output to 1 token since we only want the response"
   ],
   "id": "5cd61717ff8f495e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Y\n",
      "==========\n",
      "Prompt: 165 tokens, 296.943 tokens-per-sec\n",
      "Generation: 1 tokens, 319.102 tokens-per-sec\n",
      "Peak memory: 11.602 GB\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "13e7d047dd30900f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T16:09:02.772364Z",
     "start_time": "2025-01-21T16:09:02.768493Z"
    }
   },
   "cell_type": "code",
   "source": "print(response)",
   "id": "b843061fa540b712",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preparing the Trolley Problem dataset",
   "id": "58a6faa5193c2352"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The dataset will come from online experiments conducted using the jsPsych plugin developed by [Younes Strittmatter](https://github.com/younesStrittmatter/sweet-jsPsych/tree/main/plugins/trolley-problem). I created an online version of this experiment running on cognition.run, which makes it easy to run online experiments. If you want to run the experiment yourself, go here: https://lcxaoiwo9j.cognition.run/ and follow the instructions.\n",
    "\n",
    "Alternatively, you can install the trolley-problem plugin and run it locally. For more information, see https://github.com/younesStrittmatter/sweet-jsPsych/blob/main/plugins/trolley-problem/examples/example.html.\n",
    "\n",
    "The experiment outputs data in JSON format, which will convert to text prompts for the fine-tuning dataset. Let's load the experiment json file and see what it looks like:"
   ],
   "id": "43362a737d11b884"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T13:52:08.928937Z",
     "start_time": "2025-01-22T13:52:08.923195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "data_path = \"data/trolley_problem\"\n",
    "\n",
    "with open(f\"{data_path}/exp.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "    prompts = []\n",
    "\n",
    "    for trial in data:\n",
    "        main_count = len(trial['main_track'])\n",
    "        main_count_phrase1 = f'{main_count} people' if main_count > 1 else 'one person'\n",
    "        main_count_phrase2 = f'are {main_count_phrase1}' if main_count > 1 else 'is one person'\n",
    "\n",
    "        side_track = len(trial['side_track'])\n",
    "        side_track_phrase1 = f'{side_track} people' if side_track > 1 else 'one person'\n",
    "        side_track_phrase2 = f'are {side_track} people' if side_track > 1 else 'is one person'\n",
    "\n",
    "        prompt = f\"You are standing by the railroad tracks when you notice an empty boxcar rolling out of control. It is moving so fast that anyone it hits will die. Ahead on the main track {main_count_phrase2}. There {side_track_phrase2} standing on a side track that doesn't rejoin the main track. If you do nothing, the boxcar will hit the {main_count_phrase1} on the main track, but it will not hit the {side_track_phrase1} on the side track. If you flip a switch next to you, it will divert the boxcar to the side track where it will hit the {side_track_phrase1}, and not hit the {main_count_phrase1} on the main track. Respond with N to do nothing, or F to flip the switch.\"\n",
    "\n",
    "        prompt = f\"{prompt}\\n\\nMain track includes the following: {trial['main_track']}. Side track includes the following: {trial['side_track']}. You choose <<\"\n",
    "\n",
    "        completion = f\"{'F' if trial['action'] == 'flip' else 'N'}>>.\"\n",
    "\n",
    "        # print(prompt)\n",
    "        prompts.append({'text': prompt + completion})\n",
    "    print(prompts)"
   ],
   "id": "40cc6f952484652d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': \"You are standing by the railroad tracks when you notice an empty boxcar rolling out of control. It is moving so fast that anyone it hits will die. Ahead on the main track is one person. There is one person standing on a side track that doesn't rejoin the main track. If you do nothing, the boxcar will hit the one person on the main track, but it will not hit the one person on the side track. If you flip a switch next to you, it will divert the boxcar to the side track where it will hit the one person, and not hit the one person on the main track. Respond with N to do nothing, or F to flip the switch.\\n\\nMain track includes the following: [{'gender': 'male', 'body_type': 'business', 'skin': 'white'}]. Side track includes the following: [{'gender': 'female', 'body_type': 'pregnant', 'skin': 'black'}]. You choose <<F>>.\"}, {'text': \"You are standing by the railroad tracks when you notice an empty boxcar rolling out of control. It is moving so fast that anyone it hits will die. Ahead on the main track are 2 people. There is one person standing on a side track that doesn't rejoin the main track. If you do nothing, the boxcar will hit the 2 people on the main track, but it will not hit the one person on the side track. If you flip a switch next to you, it will divert the boxcar to the side track where it will hit the one person, and not hit the 2 people on the main track. Respond with N to do nothing, or F to flip the switch.\\n\\nMain track includes the following: [{'gender': 'female', 'body_type': 'casual', 'skin': 'white'}, {'gender': 'female', 'body_type': 'elderly', 'skin': 'brown'}]. Side track includes the following: [{'gender': 'female', 'body_type': 'pregnant', 'skin': 'black'}]. You choose <<N>>.\"}, {'text': \"You are standing by the railroad tracks when you notice an empty boxcar rolling out of control. It is moving so fast that anyone it hits will die. Ahead on the main track are 2 people. There is one person standing on a side track that doesn't rejoin the main track. If you do nothing, the boxcar will hit the 2 people on the main track, but it will not hit the one person on the side track. If you flip a switch next to you, it will divert the boxcar to the side track where it will hit the one person, and not hit the 2 people on the main track. Respond with N to do nothing, or F to flip the switch.\\n\\nMain track includes the following: [{'gender': 'female', 'body_type': 'pregnant', 'skin': 'brown'}, {'gender': 'male', 'body_type': 'elderly', 'skin': 'white'}]. Side track includes the following: [{'gender': 'female', 'body_type': 'pregnant', 'skin': 'black'}]. You choose <<N>>.\"}, {'text': \"You are standing by the railroad tracks when you notice an empty boxcar rolling out of control. It is moving so fast that anyone it hits will die. Ahead on the main track is one person. There is one person standing on a side track that doesn't rejoin the main track. If you do nothing, the boxcar will hit the one person on the main track, but it will not hit the one person on the side track. If you flip a switch next to you, it will divert the boxcar to the side track where it will hit the one person, and not hit the one person on the main track. Respond with N to do nothing, or F to flip the switch.\\n\\nMain track includes the following: [{'gender': 'female', 'body_type': 'pregnant', 'skin': 'black'}]. Side track includes the following: [{'gender': 'male', 'body_type': 'business', 'skin': 'white'}]. You choose <<F>>.\"}, {'text': \"You are standing by the railroad tracks when you notice an empty boxcar rolling out of control. It is moving so fast that anyone it hits will die. Ahead on the main track are 2 people. There is one person standing on a side track that doesn't rejoin the main track. If you do nothing, the boxcar will hit the 2 people on the main track, but it will not hit the one person on the side track. If you flip a switch next to you, it will divert the boxcar to the side track where it will hit the one person, and not hit the 2 people on the main track. Respond with N to do nothing, or F to flip the switch.\\n\\nMain track includes the following: [{'gender': 'male', 'body_type': 'casual', 'skin': 'white'}, {'gender': 'male', 'body_type': 'casual', 'skin': 'white'}]. Side track includes the following: [{'gender': 'male', 'body_type': 'casual', 'skin': 'white'}]. You choose <<F>>.\"}, {'text': \"You are standing by the railroad tracks when you notice an empty boxcar rolling out of control. It is moving so fast that anyone it hits will die. Ahead on the main track are 2 people. There is one person standing on a side track that doesn't rejoin the main track. If you do nothing, the boxcar will hit the 2 people on the main track, but it will not hit the one person on the side track. If you flip a switch next to you, it will divert the boxcar to the side track where it will hit the one person, and not hit the 2 people on the main track. Respond with N to do nothing, or F to flip the switch.\\n\\nMain track includes the following: [{'gender': 'male', 'body_type': 'casual', 'skin': 'black'}, {'gender': 'male', 'body_type': 'casual', 'skin': 'brown'}]. Side track includes the following: [{'gender': 'male', 'body_type': 'casual', 'skin': 'white'}]. You choose <<F>>.\"}, {'text': \"You are standing by the railroad tracks when you notice an empty boxcar rolling out of control. It is moving so fast that anyone it hits will die. Ahead on the main track is one person. There are 2 people standing on a side track that doesn't rejoin the main track. If you do nothing, the boxcar will hit the one person on the main track, but it will not hit the 2 people on the side track. If you flip a switch next to you, it will divert the boxcar to the side track where it will hit the 2 people, and not hit the one person on the main track. Respond with N to do nothing, or F to flip the switch.\\n\\nMain track includes the following: [{'gender': 'male', 'body_type': 'casual', 'skin': 'white'}]. Side track includes the following: [{'gender': 'male', 'body_type': 'casual', 'skin': 'white'}, {'gender': 'male', 'body_type': 'casual', 'skin': 'white'}]. You choose <<N>>.\"}, {'text': \"You are standing by the railroad tracks when you notice an empty boxcar rolling out of control. It is moving so fast that anyone it hits will die. Ahead on the main track are 3 people. There is one person standing on a side track that doesn't rejoin the main track. If you do nothing, the boxcar will hit the 3 people on the main track, but it will not hit the one person on the side track. If you flip a switch next to you, it will divert the boxcar to the side track where it will hit the one person, and not hit the 3 people on the main track. Respond with N to do nothing, or F to flip the switch.\\n\\nMain track includes the following: [{'gender': 'female', 'body_type': 'pregnant', 'skin': 'white'}, {'gender': 'female', 'body_type': 'pregnant', 'skin': 'white'}, {'gender': 'female', 'body_type': 'pregnant', 'skin': 'white'}]. Side track includes the following: [{'gender': 'female', 'body_type': 'pregnant', 'skin': 'black'}]. You choose <<F>>.\"}, {'text': \"You are standing by the railroad tracks when you notice an empty boxcar rolling out of control. It is moving so fast that anyone it hits will die. Ahead on the main track is one person. There are 3 people standing on a side track that doesn't rejoin the main track. If you do nothing, the boxcar will hit the one person on the main track, but it will not hit the 3 people on the side track. If you flip a switch next to you, it will divert the boxcar to the side track where it will hit the 3 people, and not hit the one person on the main track. Respond with N to do nothing, or F to flip the switch.\\n\\nMain track includes the following: [{'gender': 'female', 'body_type': 'pregnant', 'skin': 'brown'}]. Side track includes the following: [{'gender': 'female', 'body_type': 'pregnant', 'skin': 'white'}, {'gender': 'female', 'body_type': 'pregnant', 'skin': 'white'}, {'gender': 'female', 'body_type': 'pregnant', 'skin': 'white'}]. You choose <<N>>.\"}, {'text': \"You are standing by the railroad tracks when you notice an empty boxcar rolling out of control. It is moving so fast that anyone it hits will die. Ahead on the main track is one person. There is one person standing on a side track that doesn't rejoin the main track. If you do nothing, the boxcar will hit the one person on the main track, but it will not hit the one person on the side track. If you flip a switch next to you, it will divert the boxcar to the side track where it will hit the one person, and not hit the one person on the main track. Respond with N to do nothing, or F to flip the switch.\\n\\nMain track includes the following: [{'gender': 'male', 'body_type': 'business', 'skin': 'white'}]. Side track includes the following: [{'gender': 'female', 'body_type': 'pregnant', 'skin': 'black'}]. You choose <<F>>.\"}]\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's write the prompts to a json-l file that we can use for fine-tuning:",
   "id": "5534538b85daa8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T07:38:53.522560Z",
     "start_time": "2025-01-22T07:38:53.519258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_len = len(prompts)\n",
    "val_len = int(total_len * 0.2) # 20% validation set\n",
    "\n",
    "with open(f\"{data_path}/train.jsonl\", \"w\") as f:\n",
    "    for prompt in prompts[:-val_len]:\n",
    "        json.dump(prompt, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open(f\"{data_path}/valid.jsonl\", \"w\") as f:\n",
    "    for prompt in prompts[-val_len:]:\n",
    "        json.dump(prompt, f)\n",
    "        f.write('\\n')"
   ],
   "id": "69d27c7d3514d5c7",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Prompting before fine-tuning\n",
    "\n",
    "First we try the default model of MLX-LM with the first prompt from the dataset:"
   ],
   "id": "e712989c0f23e606"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T16:09:10.210406Z",
     "start_time": "2025-01-21T16:09:07.327812Z"
    }
   },
   "cell_type": "code",
   "source": "!mlx_lm.generate --prompt f\"{prompts[0]['prompt']}\"",
   "id": "a8d7155e4f3a959c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|███████████████████████| 6/6 [00:00<00:00, 108942.96it/s]\r\n",
      "==========\r\n",
      "F\r\n",
      "==========\r\n",
      "Prompt: 233 tokens, 327.493 tokens-per-sec\r\n",
      "Generation: 2 tokens, 133.402 tokens-per-sec\r\n",
      "Peak memory: 1.964 GB\r\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's try the same prompt with the converted Centaur model:",
   "id": "931f84c074bcb476"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T16:09:13.038954Z",
     "start_time": "2025-01-21T16:09:12.169903Z"
    }
   },
   "cell_type": "code",
   "source": "response = generate(model, tokenizer, prompt=prompts[0]['prompt'], verbose=True, max_tokens=1)",
   "id": "544cab1fd4e9c0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "N\n",
      "==========\n",
      "Prompt: 198 tokens, 309.977 tokens-per-sec\n",
      "Generation: 1 tokens, 589.666 tokens-per-sec\n",
      "Peak memory: 11.602 GB\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fine-tuning\n",
    "\n",
    "Now finally we can fine-tune the model on the Trolley Problem dataset. We will use the `mlx_lm.train` command to fine-tune the model. We will use the `train.jsonl` file we created earlier as the training data. We will also use the `--save` flag to save the model after training."
   ],
   "id": "cb5803772676527a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T07:50:54.417716Z",
     "start_time": "2025-01-22T07:41:19.219859Z"
    }
   },
   "cell_type": "code",
   "source": "!mlx_lm.lora --model ./centaur8b --train --fine-tune-type lora --data ./data/trolley_problem --iters 100",
   "id": "98ab4d58cc7c6521",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\r\n",
      "Loading datasets\r\n",
      "Training\r\n",
      "Trainable parameters: 0.042% (3.408M/8030.261M)\r\n",
      "Starting training..., iters: 100\r\n",
      "Iter 1: Val loss 1.814, Val took 3.555s\r\n",
      "Iter 10: Train loss 1.507, Learning Rate 1.000e-05, It/sec 0.172, Tokens/sec 152.199, Trained Tokens 8872, Peak mem 13.518 GB\r\n",
      "Iter 20: Train loss 0.643, Learning Rate 1.000e-05, It/sec 0.172, Tokens/sec 153.904, Trained Tokens 17800, Peak mem 13.518 GB\r\n",
      "Iter 30: Train loss 0.199, Learning Rate 1.000e-05, It/sec 0.176, Tokens/sec 155.293, Trained Tokens 26632, Peak mem 13.518 GB\r\n",
      "Iter 40: Train loss 0.106, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 158.630, Trained Tokens 35600, Peak mem 13.518 GB\r\n",
      "Iter 50: Train loss 0.070, Learning Rate 1.000e-05, It/sec 0.177, Tokens/sec 158.237, Trained Tokens 44528, Peak mem 13.518 GB\r\n",
      "Iter 60: Train loss 0.055, Learning Rate 1.000e-05, It/sec 0.180, Tokens/sec 159.440, Trained Tokens 53400, Peak mem 13.518 GB\r\n",
      "Iter 70: Train loss 0.049, Learning Rate 1.000e-05, It/sec 0.175, Tokens/sec 156.595, Trained Tokens 62368, Peak mem 13.518 GB\r\n",
      "Iter 80: Train loss 0.046, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 159.520, Trained Tokens 71200, Peak mem 13.518 GB\r\n",
      "Iter 90: Train loss 0.043, Learning Rate 1.000e-05, It/sec 0.179, Tokens/sec 159.476, Trained Tokens 80128, Peak mem 13.518 GB\r\n",
      "Iter 100: Val loss 0.081, Val took 2.778s\r\n",
      "Iter 100: Train loss 0.041, Learning Rate 1.000e-05, It/sec 1.725, Tokens/sec 1530.187, Trained Tokens 89000, Peak mem 13.518 GB\r\n",
      "Iter 100: Saved adapter weights to adapters/adapters.safetensors and adapters/0000100_adapters.safetensors.\r\n",
      "Saved final weights to adapters/adapters.safetensors.\r\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T08:12:52.621365Z",
     "start_time": "2025-01-22T08:12:47.093333Z"
    }
   },
   "cell_type": "code",
   "source": "!mlx_lm.lora --model centaur8b --adapter-path adapters --data ./data/trolley_problem --test",
   "id": "816f84727f83f0e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\r\n",
      "Loading datasets\r\n",
      "Testing\r\n",
      "Test loss 0.079, Test ppl 1.082.\r\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T08:10:26.250940Z",
     "start_time": "2025-01-22T08:10:26.249026Z"
    }
   },
   "cell_type": "code",
   "source": "prompt_test = '{\"text\": \"You are standing by the railroad tracks when you notice an empty boxcar rolling out of control. It is moving so fast that anyone it hits will die. Ahead on the main track are 3 people. There is one person standing on a side track that doesn\\'t rejoin the main track. If you do nothing, the boxcar will hit the 3 people on the main track, but it will not hit the one person on the side track. If you flip a switch next to you, it will divert the boxcar to the side track where it will hit the one person, and not hit the 3 people on the main track. Respond with N to do nothing, or F to flip the switch.\\n\\nMain track includes the following: [{\\'gender\\': \\'female\\', \\'body_type\\': \\'pregnant\\', \\'skin\\': \\'white\\'}, {\\'gender\\': \\'female\\', \\'body_type\\': \\'pregnant\\', \\'skin\\': \\'white\\'}, {\\'gender\\': \\'female\\', \\'body_type\\': \\'pregnant\\', \\'skin\\': \\'white\\'}]. Side track includes the following: [{\\'gender\\': \\'female\\', \\'body_type\\': \\'pregnant\\', \\'skin\\': \\'black\\'}]. You choose <<\"}'",
   "id": "4e58baf383af775",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T08:14:51.366858Z",
     "start_time": "2025-01-22T08:14:51.365144Z"
    }
   },
   "cell_type": "code",
   "source": "prompt_test = prompts[0]['text'][:-4]",
   "id": "a116e46b7cbed455",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T08:14:54.800536Z",
     "start_time": "2025-01-22T08:14:52.416721Z"
    }
   },
   "cell_type": "code",
   "source": "!mlx_lm.generate --model ./centaur8b --adapter-path ./adapters --prompt f\"{prompt_test}\"",
   "id": "b3f821c608685091",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\r\n",
      "F>>.\r\n",
      "==========\r\n",
      "Prompt: 199 tokens, 290.104 tokens-per-sec\r\n",
      "Generation: 4 tokens, 17.915 tokens-per-sec\r\n",
      "Peak memory: 4.693 GB\r\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "response = generate(model, tokenizer, prompt=prompts[0]['prompt'], verbose=True, max_tokens=1))",
   "id": "a0b223613cd0c8fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T16:33:04.046793Z",
     "start_time": "2025-01-21T16:33:01.082875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from mlx_lm import lora\n",
    "import mlx.optimizers as optim\n",
    "from mlx_lm.tuner.trainer import TrainingArgs\n",
    "from mlx_lm.tuner.datasets import load_local_dataset\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "opt = optim.Adam(\n",
    "        learning_rate=(\n",
    "            0.0001\n",
    "        ))\n",
    "train, valid, test = load_local_dataset(Path(data_path), tokenizer)\n",
    "\n",
    "lora.train(model, tokenizer, optimizer=opt, train_dataset=train, val_dataset=valid, args=TrainingArgs())"
   ],
   "id": "d17630d30db0d8eb",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 13\u001B[0m\n\u001B[1;32m      8\u001B[0m opt \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(\n\u001B[1;32m      9\u001B[0m         learning_rate\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m     10\u001B[0m             \u001B[38;5;241m0.0001\u001B[39m\n\u001B[1;32m     11\u001B[0m         ))\n\u001B[1;32m     12\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmeta-llama/Llama-3.1-8B\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 13\u001B[0m train, valid, test \u001B[38;5;241m=\u001B[39m \u001B[43mload_local_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mPath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_path\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m lora\u001B[38;5;241m.\u001B[39mtrain(model, tokenizer, optimizer\u001B[38;5;241m=\u001B[39mopt, train_dataset\u001B[38;5;241m=\u001B[39mtrain, val_dataset\u001B[38;5;241m=\u001B[39mvalid, args\u001B[38;5;241m=\u001B[39mTrainingArgs())\n",
      "File \u001B[0;32m~/.pyenv/versions/centaur-finetuning/lib/python3.12/site-packages/mlx_lm/tuner/datasets.py:120\u001B[0m, in \u001B[0;36mload_local_dataset\u001B[0;34m(data_path, tokenizer, prompt_feature, completion_feature)\u001B[0m\n\u001B[1;32m    117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m create_dataset(data, tokenizer, prompt_feature, completion_feature)\n\u001B[1;32m    119\u001B[0m names \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalid\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 120\u001B[0m train, valid, test \u001B[38;5;241m=\u001B[39m [\u001B[43mload_subset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_path\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mn\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.jsonl\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m n \u001B[38;5;129;01min\u001B[39;00m names]\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m train, valid, test\n",
      "File \u001B[0;32m~/.pyenv/versions/centaur-finetuning/lib/python3.12/site-packages/mlx_lm/tuner/datasets.py:117\u001B[0m, in \u001B[0;36mload_local_dataset.<locals>.load_subset\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m fid:\n\u001B[1;32m    116\u001B[0m     data \u001B[38;5;241m=\u001B[39m [json\u001B[38;5;241m.\u001B[39mloads(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m fid]\n\u001B[0;32m--> 117\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcreate_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt_feature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompletion_feature\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/centaur-finetuning/lib/python3.12/site-packages/mlx_lm/tuner/datasets.py:96\u001B[0m, in \u001B[0;36mcreate_dataset\u001B[0;34m(data, tokenizer, prompt_feature, completion_feature)\u001B[0m\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ChatDataset(data, tokenizer)\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m prompt_feature \u001B[38;5;129;01min\u001B[39;00m sample \u001B[38;5;129;01mand\u001B[39;00m completion_feature \u001B[38;5;129;01min\u001B[39;00m sample:\n\u001B[0;32m---> 96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mCompletionsDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt_feature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompletion_feature\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m sample:\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Dataset(data, tokenizer)\n",
      "File \u001B[0;32m~/.pyenv/versions/centaur-finetuning/lib/python3.12/site-packages/mlx_lm/tuner/datasets.py:68\u001B[0m, in \u001B[0;36mCompletionsDataset.__init__\u001B[0;34m(self, data, tokenizer, prompt_key, completion_key)\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     62\u001B[0m     data: List[Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     65\u001B[0m     completion_key: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m     66\u001B[0m ):\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m---> 68\u001B[0m         \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_chat_template\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     69\u001B[0m \u001B[43m            \u001B[49m\u001B[43m[\u001B[49m\n\u001B[1;32m     70\u001B[0m \u001B[43m                \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrole\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontent\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m[\u001B[49m\u001B[43mprompt_key\u001B[49m\u001B[43m]\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     71\u001B[0m \u001B[43m                \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrole\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43massistant\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontent\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcompletion_key\u001B[49m\u001B[43m]\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     72\u001B[0m \u001B[43m            \u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m data\n\u001B[1;32m     75\u001B[0m     ]\n",
      "File \u001B[0;32m~/.pyenv/versions/centaur-finetuning/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1621\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.apply_chat_template\u001B[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001B[0m\n\u001B[1;32m   1618\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tokenizer_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1619\u001B[0m     tokenizer_kwargs \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m-> 1621\u001B[0m chat_template \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_chat_template\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchat_template\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1623\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_assistant_tokens_mask \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m re\u001B[38;5;241m.\u001B[39msearch(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m-?\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms*generation\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms*-?\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m}\u001B[39m\u001B[38;5;124m\"\u001B[39m, chat_template):\n\u001B[1;32m   1624\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarning_once(\n\u001B[1;32m   1625\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreturn_assistant_tokens_mask==True but chat template does not contain `\u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;132;01m% g\u001B[39;00m\u001B[38;5;124meneration \u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m}` keyword.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1626\u001B[0m     )\n",
      "File \u001B[0;32m~/.pyenv/versions/centaur-finetuning/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1789\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.get_chat_template\u001B[0;34m(self, chat_template, tools)\u001B[0m\n\u001B[1;32m   1787\u001B[0m         chat_template \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchat_template\n\u001B[1;32m   1788\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1789\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1790\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1791\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124margument was passed! For information about writing templates and setting the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1792\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokenizer.chat_template attribute, please see the documentation at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1793\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1794\u001B[0m         )\n\u001B[1;32m   1796\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m chat_template\n",
      "\u001B[0;31mValueError\u001B[0m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
